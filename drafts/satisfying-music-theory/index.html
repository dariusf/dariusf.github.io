<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://dariusf.github.io/images/favicon.png><title>Satisfying Music Theory | Darius Foo</title>
<meta name=title content="Satisfying Music Theory"><meta name=description content='





This post is about my experiments in generating music using SMT.
As a teaser, here is a procedurally-generated harmonization of the first line of 
Joy to the World.
Given the melody (in the soprano voice), the system produces the other three voices in a way that makes harmonic sense.


X: 1
T: Joy to the World
L: 1/16
K: C
M: 4/4
Q: 1/4=110
V: sop name="S"
V: alto name="A"
V: tenor name="T" clef=treble-8
V: bass name="B" clef=bass middle=d transpose=-24
%
V: sop
c4 B3 A1 G6 F2 E4 D4 C6 G2 A6 A2 B6 B2 c6
V: alto
e4 d4 G6 d2 G6 G6 c6 F4 A2 e2 e1 e4 B1 c6
V: tenor
A4 D4 G6 A2 E1 E3 D4 E4 E6 A6 G4 G1 G3 A6
V: bass
"vi" A4 "V" G3 "ii" F1 "I" G6 "ii" A1 "iii" d1 "V" G6 "I" G6 "I" c6 "vi" d6 "ii" G2 "iii" B3 "iii" G3 "vi" A6




More examples here. The rest of the post describes the journey (or at least the first part of it, since it appears to be far from over!).'><meta name=keywords content><meta property="og:url" content="https://dariusf.github.io/drafts/satisfying-music-theory/"><meta property="og:site_name" content="Darius Foo"><meta property="og:title" content="Satisfying Music Theory"><meta property="og:description" content='This post is about my experiments in generating music using SMT.
As a teaser, here is a procedurally-generated harmonization of the first line of Joy to the World. Given the melody (in the soprano voice), the system produces the other three voices in a way that makes harmonic sense.
X: 1 T: Joy to the World L: 1/16 K: C M: 4/4 Q: 1/4=110 V: sop name="S" V: alto name="A" V: tenor name="T" clef=treble-8 V: bass name="B" clef=bass middle=d transpose=-24 % V: sop c4 B3 A1 G6 F2 E4 D4 C6 G2 A6 A2 B6 B2 c6 V: alto e4 d4 G6 d2 G6 G6 c6 F4 A2 e2 e1 e4 B1 c6 V: tenor A4 D4 G6 A2 E1 E3 D4 E4 E6 A6 G4 G1 G3 A6 V: bass "vi" A4 "V" G3 "ii" F1 "I" G6 "ii" A1 "iii" d1 "V" G6 "I" G6 "I" c6 "vi" d6 "ii" G2 "iii" B3 "iii" G3 "vi" A6 More examples here. The rest of the post describes the journey (or at least the first part of it, since it appears to be far from over!).'><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="drafts"><meta property="article:published_time" content="2022-12-30T10:41:06+08:00"><meta property="article:modified_time" content="2022-12-30T10:41:06+08:00"><meta property="og:image" content="https://dariusf.github.io/images/favicon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dariusf.github.io/images/favicon.png"><meta name=twitter:title content="Satisfying Music Theory"><meta name=twitter:description content='This post is about my experiments in generating music using SMT.
As a teaser, here is a procedurally-generated harmonization of the first line of Joy to the World. Given the melody (in the soprano voice), the system produces the other three voices in a way that makes harmonic sense.
X: 1 T: Joy to the World L: 1/16 K: C M: 4/4 Q: 1/4=110 V: sop name="S" V: alto name="A" V: tenor name="T" clef=treble-8 V: bass name="B" clef=bass middle=d transpose=-24 % V: sop c4 B3 A1 G6 F2 E4 D4 C6 G2 A6 A2 B6 B2 c6 V: alto e4 d4 G6 d2 G6 G6 c6 F4 A2 e2 e1 e4 B1 c6 V: tenor A4 D4 G6 A2 E1 E3 D4 E4 E6 A6 G4 G1 G3 A6 V: bass "vi" A4 "V" G3 "ii" F1 "I" G6 "ii" A1 "iii" d1 "V" G6 "I" G6 "I" c6 "vi" d6 "ii" G2 "iii" B3 "iii" G3 "vi" A6 More examples here. The rest of the post describes the journey (or at least the first part of it, since it appears to be far from over!).'><meta itemprop=name content="Satisfying Music Theory"><meta itemprop=description content='This post is about my experiments in generating music using SMT.
As a teaser, here is a procedurally-generated harmonization of the first line of Joy to the World. Given the melody (in the soprano voice), the system produces the other three voices in a way that makes harmonic sense.
X: 1 T: Joy to the World L: 1/16 K: C M: 4/4 Q: 1/4=110 V: sop name="S" V: alto name="A" V: tenor name="T" clef=treble-8 V: bass name="B" clef=bass middle=d transpose=-24 % V: sop c4 B3 A1 G6 F2 E4 D4 C6 G2 A6 A2 B6 B2 c6 V: alto e4 d4 G6 d2 G6 G6 c6 F4 A2 e2 e1 e4 B1 c6 V: tenor A4 D4 G6 A2 E1 E3 D4 E4 E6 A6 G4 G1 G3 A6 V: bass "vi" A4 "V" G3 "ii" F1 "I" G6 "ii" A1 "iii" d1 "V" G6 "I" G6 "I" c6 "vi" d6 "ii" G2 "iii" B3 "iii" G3 "vi" A6 More examples here. The rest of the post describes the journey (or at least the first part of it, since it appears to be far from over!).'><meta itemprop=datePublished content="2022-12-30T10:41:06+08:00"><meta itemprop=dateModified content="2022-12-30T10:41:06+08:00"><meta itemprop=wordCount content="2418"><meta itemprop=image content="https://dariusf.github.io/images/favicon.png"><meta name=referrer content="no-referrer-when-downgrade"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Fira+Code"><style>@media(prefers-color-scheme:light){:root{--bright-text-color:#222;--link-color:#3273dc;--link-visited-color:#6e4bbe;--background-color:#fff;--text-color:#444;--faded-text-color:#777;--blockquote-text-color:var(--bright-text-color);--faint-color:#ccc}.chroma{background-color:#fff}.chroma .x{}.chroma .err{color:#000}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#a90d91}.chroma .kc{color:#a90d91}.chroma .kd{color:#a90d91}.chroma .kn{color:#a90d91}.chroma .kp{color:#a90d91}.chroma .kr{color:#a90d91}.chroma .kt{color:#a90d91}.chroma .n{color:#000}.chroma .na{color:#836c28}.chroma .nb{color:#a90d91}.chroma .bp{color:#5b269a}.chroma .nc{color:#3f6e75}.chroma .no{color:#000}.chroma .nd{color:#000}.chroma .ni{color:#000}.chroma .ne{color:#000}.chroma .nf{color:#000}.chroma .fm{color:#000}.chroma .nl{color:#000}.chroma .nn{color:#000}.chroma .nx{color:#000}.chroma .py{color:#000}.chroma .nt{color:#000}.chroma .nv{color:#000}.chroma .vc{color:#000}.chroma .vg{color:#000}.chroma .vi{color:#000}.chroma .vm{color:#000}.chroma .l{color:#1c01ce}.chroma .ld{color:#1c01ce}.chroma .s{color:#c41a16}.chroma .sa{color:#c41a16}.chroma .sb{color:#c41a16}.chroma .sc{color:#2300ce}.chroma .dl{color:#c41a16}.chroma .sd{color:#c41a16}.chroma .s2{color:#c41a16}.chroma .se{color:#c41a16}.chroma .sh{color:#c41a16}.chroma .si{color:#c41a16}.chroma .sx{color:#c41a16}.chroma .sr{color:#c41a16}.chroma .s1{color:#c41a16}.chroma .ss{color:#c41a16}.chroma .m{color:#1c01ce}.chroma .mb{color:#1c01ce}.chroma .mf{color:#1c01ce}.chroma .mh{color:#1c01ce}.chroma .mi{color:#1c01ce}.chroma .il{color:#1c01ce}.chroma .mo{color:#1c01ce}.chroma .o{color:#000}.chroma .ow{color:#000}.chroma .p{}.chroma .c{color:#177500}.chroma .ch{color:#177500}.chroma .cm{color:#177500}.chroma .c1{color:#177500}.chroma .cs{color:#177500}.chroma .cp{color:#633820}.chroma .cpf{color:#633820}.chroma .g{}.chroma .gd{}.chroma .ge{}.chroma .gr{}.chroma .gh{}.chroma .gi{}.chroma .go{}.chroma .gp{}.chroma .gs{}.chroma .gu{}.chroma .gt{}.chroma .gl{}.chroma .w{}}@media(prefers-color-scheme:dark){:root{--bright-text-color:#eee;--link-color:#8cc2dd;--link-visited-color:#b9a9e0;--background-color:#333;--text-color:#ddd;--faded-text-color:#aaa;--slightly-dimmer-text-color:#ccc;--blockquote-text-color:var(--slightly-dimmer-text-color);--faint-color:#666;color-scheme:dark}img.theme-affected{filter:invert(.8)}.chroma{color:#e2e4e5;background-color:#282a36}.chroma .x{}.chroma .err{color:#ff5c57}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#ff6ac1}.chroma .kc{color:#ff6ac1}.chroma .kd{color:#ff5c57}.chroma .kn{color:#ff6ac1}.chroma .kp{color:#ff6ac1}.chroma .kr{color:#ff6ac1}.chroma .kt{color:#9aedfe}.chroma .n{}.chroma .na{color:#57c7ff}.chroma .nb{color:#ff5c57}.chroma .bp{}.chroma .nc{color:#f3f99d}.chroma .no{color:#ff9f43}.chroma .nd{color:#ff9f43}.chroma .ni{}.chroma .ne{}.chroma .nf{color:#57c7ff}.chroma .fm{}.chroma .nl{color:#ff5c57}.chroma .nn{}.chroma .nx{}.chroma .py{}.chroma .nt{color:#ff6ac1}.chroma .nv{color:#ff5c57}.chroma .vc{color:#ff5c57}.chroma .vg{color:#ff5c57}.chroma .vi{color:#ff5c57}.chroma .vm{}.chroma .l{}.chroma .ld{}.chroma .s{color:#5af78e}.chroma .sa{color:#5af78e}.chroma .sb{color:#5af78e}.chroma .sc{color:#5af78e}.chroma .dl{color:#5af78e}.chroma .sd{color:#5af78e}.chroma .s2{color:#5af78e}.chroma .se{color:#5af78e}.chroma .sh{color:#5af78e}.chroma .si{color:#5af78e}.chroma .sx{color:#5af78e}.chroma .sr{color:#5af78e}.chroma .s1{color:#5af78e}.chroma .ss{color:#5af78e}.chroma .m{color:#ff9f43}.chroma .mb{color:#ff9f43}.chroma .mf{color:#ff9f43}.chroma .mh{color:#ff9f43}.chroma .mi{color:#ff9f43}.chroma .il{color:#ff9f43}.chroma .mo{color:#ff9f43}.chroma .o{color:#ff6ac1}.chroma .ow{color:#ff6ac1}.chroma .p{}.chroma .c{color:#78787e}.chroma .ch{color:#78787e}.chroma .cm{color:#78787e}.chroma .c1{color:#78787e}.chroma .cs{color:#78787e}.chroma .cp{color:#78787e}.chroma .cpf{color:#78787e}.chroma .g{}.chroma .gd{color:#ff5c57}.chroma .ge{text-decoration:underline}.chroma .gr{color:#ff5c57}.chroma .gh{font-weight:700}.chroma .gi{font-weight:700}.chroma .go{color:#43454f}.chroma .gp{}.chroma .gs{font-style:italic}.chroma .gu{font-weight:700}.chroma .gt{}.chroma .gl{text-decoration:underline}.chroma .w{}}:root{--heading-font:ui-rounded, 'Hiragino Maru Gothic ProN', Quicksand, Comfortaa, Manjari, 'Arial Rounded MT', 'Arial Rounded MT Bold', Calibri, source-sans-pro, sans-serif;--text-font-size:17px;--text-font:system-ui, sans-serif}*{margin:0}body{}main{margin-top:.75em}sup{vertical-align:top;font-size:.7em}p{margin-bottom:1em}html{overflow-x:hidden;margin-right:calc(-1 * (100vw - 100%))}body{font-family:var(--text-font);font-size:var(--text-font-size);margin:auto;padding:20px;max-width:720px;text-align:left;background-color:var(--background-color);color:var(--text-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5}h1,h2,h3,h4,h5,h6{font-family:var(--heading-font);color:var(--bright-text-color);font-weight:400}.blog-content h1,.blog-content h2,.blog-content h3,.blog-content h4,.blog-content h5,.blog-content h6{margin-top:.5em}hr,ul,ol{margin-bottom:.5em}div>iframe{margin-bottom:.5em}.blog-timestamp{font-size:.9em}.blog-content{margin-top:1.5em}a{color:var(--link-color);text-decoration:none}.title{color:var(--text-color);font-size:1.5em;font-family:var(--heading-font);margin-right:10px}nav a{margin-right:8px;font-family:var(--heading-font)}.paper-item svg{width:16px;vertical-align:text-bottom}.menuactive{text-decoration:underline;text-decoration-thickness:2px}table{width:100%}img{max-width:100%;margin:auto;margin-bottom:.5em;display:block}code{padding:2px 5px;margin-bottom:1em;font-family:Fira Code,monospace;font-size:14px;line-height:1.4}pre code{display:block;padding:20px;white-space:pre-wrap;overflow-x:auto}pre{border-radius:10px}p code,ol code,ul code,summary code{border-radius:4px;border:solid var(--faint-color)1px;margin:0 2px;padding:1px 2px}blockquote{border-left:1px solid #999;color:var(--blockquote-text-color);padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.deemphasize{color:var(--faded-text-color)}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px;text-align:right;margin-right:20px}ul.blog-posts li a:visited{color:var(--link-visited-color)}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script>const macros={};function render(){renderMathInElement(document.body,{trust:!0,strict:!1,throwOnError:!1,delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros})}</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=render()></script></head><body><header><a href=/><span class=title>Darius Foo</span></a><nav style=display:inline-block><a href=/blog/>Blog</a>
<a href=/research/>Research</a>
<a href=/work/>Work</a>
<a href=/other/>Other</a></nav></header><main><h1>Satisfying Music Theory</h1><div><time class=blog-timestamp datetime=2022-12-30 pubdate>30 Dec, 2022</time></div><div class=blog-content><script src=/abcjs-basic-min.js></script><link rel=stylesheet type=text/css href=/abcjs-audio.css><script src=/abcjs.js></script><p>This post is about my experiments in generating music using SMT.</p><p>As a teaser, here is a procedurally-generated harmonization of the first line of <em>Joy to the World</em>.
Given the melody (in the soprano voice), the system produces the other three voices in a way that makes harmonic sense.</p><span id=joy>X: 1
T: Joy to the World
L: 1/16
K: C
M: 4/4
Q: 1/4=110
V: sop name="S"
V: alto name="A"
V: tenor name="T" clef=treble-8
V: bass name="B" clef=bass middle=d transpose=-24
%
V: sop
c4 B3 A1 G6 F2 E4 D4 C6 G2 A6 A2 B6 B2 c6
V: alto
e4 d4 G6 d2 G6 G6 c6 F4 A2 e2 e1 e4 B1 c6
V: tenor
A4 D4 G6 A2 E1 E3 D4 E4 E6 A6 G4 G1 G3 A6
V: bass
"vi" A4 "V" G3 "ii" F1 "I" G6 "ii" A1 "iii" d1 "V" G6 "I" G6 "I" c6 "vi" d6 "ii" G2 "iii" B3 "iii" G3 "vi" A6
</span><script>renderMusicIn("joy")</script><p>More examples <a href=#examples>here</a>. The rest of the post describes the journey (or at least the first part of it, since it appears to be far from over!).</p><h1 id=procedural-music-composition>Procedural music composition</h1><p>I've always found Renaissance polyphony deeply beautiful and intriguing. As an example, here is a performance of <a href="https://www.youtube.com/watch?v=Pdv9vmGo4EE">Sicut Cervus</a>, one of the hallmark pieces of the period.</p><p>Here is a <a href="https://www.youtube.com/watch?v=0AgZuB9HuIc">visualization</a> of the piece which shows how the four voices move and interact. The voices have individual character, sometimes engage with each other, and all fit together to create cohesive harmony.</p><p>Here's another example, <a href="https://www.youtube.com/watch?v=VvK6awPV8VQ">Though Amaryllis Dance in Green</a>.</p><p>Check out the <a href="http://youtube.com/watch?v=svvJrkfm5uI">visualization</a> (watch at 2x speed). As an <a href=https://www.sonomabach.org/though-amaryllis---william-byrd.html>analysis</a> describes it,</p><blockquote><p>What makes the piece special - and very fun to hear and to sing - is the liveliness of Byrd's setting. The five parts are extremely independent of each other, each filled with syncopations and leaps, like five dancers improvising, individually but in relation to each other. It makes me think of <a href=https://www.invaluable.com/auction-lot/schomer-frank-lichtner-wisconsin-1905-2006-dancers-89-c-cf06fdea43>this wonderful painting</a> by Frank Schomer Lichtner.</p></blockquote><p>How are such intricate pieces created? Can it be done systematically?</p><p>The simple approach of choosing chords first, then choosing notes which fit wouldn't result in independent voices.
Perhaps a nice, pleasing melody would work as a base, then the other voices could be written, ensuring they work with the harmony, which now emerges from the voices.
Presumably this would get harder and harder with each voice added, as new ideas would necessitate revisions of earlier voices to admit them and keep everything cohesive.
And this would be done iteratively until the piece is complete.</p><p>This sort of interleaved and iterative process of refinement might remind one of constraint propagation algorithms, and indeed, computer-aided composition using constraint solvers is its own subfield.
The "omnidirectional" nature of the solving process points to modelling the entire thing as a single constraint satisfaction problem <a href=https://adamsmith.as/papers/fdg2013_shortcuts.pdf>for more effective propagation</a>.</p><p>At this point I was curious to see how well a simple, baseline approach with an SMT solver would work.
Writing small prototypes is always fun, just to get a feel for the workflow and requirements, and if the idea would even work at all.
SMT solvers are a staple in my area of research, so how far could I get by simply writing some constraints in Python, solving them with Z3, then rendering the result?</p><p>This turned out to be <em>way</em> more of a rabbit hole than I anticipated.</p><h1 id=vision>Vision</h1><p>Many existing systems tackle specific musical tasks,
e.g. <a href=https://github.com/blapiere/Rhythm-Box>generating rhythm</a>, <a href=http://musictech.mit.edu/sites/default/files/documents/pbchen_meng.pdf>automatic harmonization</a>, <a href=https://github.com/sprockeelsd/Melodizer>generating melody given a rhythm and chord progression</a>, <a href=https://dl.acm.org/doi/10.1145/3036290.3036295>generating a countermelody</a>, etc.</p><p>What I hoped to build was something more general, more of a logical foundation for all these tasks. A follow-up goal would then be to build a <a href=https://pure.york.ac.uk/portal/en/publications/mixed-initiative-creative-interfaces>mixed-initiative</a> composition tool on top of this foundation, perhaps even one accessible to <a href=https://computationalcreativity.net/iccc2015/proceedings/10_2Compton.pdf>casual creators</a>.</p><p>In particular, I was inspired by work on ASP-based procedural narrative generation, which encodes stories in logical form and searches for solutions to procedural generation tasks.
Section 6 of <a href=https://adamsmith.as/papers/a17-chen.pdf>this paper</a> describes how in this very general setting, changing the amount of initial input leads to many modes of interaction with the system.
Specializing them to our musical use case:</p><ol><li><em>Tabula rasa</em> generation: using the background theory (e.g. harmony and melody constraints) to generate a piece from scratch</li><li>Constrained generation: finishing an incomplete piece; satisfying a melody (harmonization), chord progression, etc.</li><li>Generation based on a partial piece: starting with a piece that satisfies constraints (e.g. a well-known classic work), remove constraints to generate variations</li></ol><p>There is a fourth use case which other systems in the typed music composition community highlight:</p><ol start=4><li>Validation: if the piece does not satisfy constraints, the violated constraints show mistakes in the piece or (more likely) inadequacies in our music theory</li></ol><h1 id=encoding>Encoding</h1><p>There are many possible logical encodings for musical scores and concepts, with different tradeoffs with respect to expressiveness and solver performance.
For example, a naive one would be to represent rhythm as a fixed grid of boolean variables, which would work for some kinds of music.
The <a href=https://strasheela.sourceforge.net/documents/TorstenAnders-PhDThesis.pdf>Strasheela thesis</a> covers numerous encoding schemes, so I'll just go over my specific choices.</p><p>Pitch is represented by bounded integer variables.</p>$$
\mathit{pitch}_p \equiv \{\, \rho_i \mid min_p \le \rho_i \le max_p,\, 0 \le i < n \,\},\, p \in \{\,S, A, T, B\,\}
$$<p>Rhythm is represented by duration variables.
Each note and chord has a duration, whose <em>start</em> is represented as an offset from the beginning of the piece.
The first duration has value 0, the last is strictly smaller than the total duration of the piece, and the sequences of durations within a part are monotonically increasing.</p>$$
\mathit{rhythm}_p \equiv \{\, d_i \mid d_0 = 0,\, d_0 < d_1 < ... < d_n < d_t,\, 0 \le i < n \}
$$<p>Chords are treated the same way, with a duration and pitch (class).
Ties and measures are not represented and are treated as a rendering concern.</p><p>Whether a note is a rest is represented by a boolean variable, which causes the pitch value to be ignored iff it is true.</p>$$
\mathit{rests}_p \equiv \{\, r_i \mid 0 \le i < n \}
$$<p>Next, melody and harmony.</p><p>To apply melodic constraints, we must know if two notes are <em>consecutive</em>, and to apply harmonic constraints, we must know if a chord and note are <em>simultaneous</em>.
In a very general setting like ours,
this information may not yet be available, since it may be produced as a result of the search.</p><p>For example, we won't know if a chord and note are simultaneous if we have not yet determined their rhythm, which may in turn be affected by harmonic constraints, as a certain rhythm may not be allowed if it results in a dissonant chord.
This is called the <em>inaccessible score context problem</em>, from Section 6.3 of the <a href=https://strasheela.sourceforge.net/documents/TorstenAnders-PhDThesis.pdf>Strasheela thesis</a>.</p><p>The solution we use is logical implication, which this representation makes easy.
A chord and note are simultaneous if their duration variables overlap, and harmony constraints are applied if the simultaneous note is not a rest.</p>$$
simultaneous_{(n,c)} \equiv d_n < \mathit{end}(d_c) \wedge d_c < \mathit{end}(d_n)
$$$$
harmony_{(n, c)} \equiv \neg r_c \wedge simultaneous_{(n,c)} \implies n \in \mathit{notes}(c)
$$<p>Two adjacent notes are consecutive if they are both not rests, and melody constraints are applied if notes are consecutive.</p>$$
consecutive_{(n_i,n_j)} \equiv \neg r_i \wedge \neg r_j
$$$$
melody_{(n_i,n_j)} \equiv consecutive_{(n_i,n_j)} \implies \mathit{abs}(p_i - p_{i+1}) \in \mathit{intervals}
$$<p>To enforce harmony constraints, we need a list of allowed pitches given a chord; a <em>relation</em> or <em>table constraint</em> in constraint programming terms.
In typical SMT fashion, we unroll everything into a big disjunction instead.</p><h1 id=false-starts>False starts</h1><p>I tried several other approaches before settling on SMT.
My initial naive encodings with Z3 had disappointing performance.
I then tried extending <a href=http://facile.recherche.enac.fr/>FaCiLe</a> with table constraints, but implementing propagators correctly is notoriously difficult and I experienced that firsthand.
Then I tried or-tools, which worked well until I encountered a <a href=https://github.com/google/or-tools/discussions/3373>limitation</a> with reified constraints.
At this point I went back to Z3 and committed to finding an efficient encoding.</p><p>The takeaway: start with SMT. There probably is an efficient SMT encoding. If there isn't (and you have confirmed this!), you will likely have to look into a custom theory solver for your domain.</p><p>A few examples of subpar encodings:</p><ul><li>Use of modulo to handle octaves - unrolling is faster</li><li>Use of a tuple of a pitch class and octave to model notes - bounded integers are faster</li><li>Use of enumerations to model pitch - bounded integers are more flexible</li><li>Unnecessary uses of if-then-else - just use disjunction</li></ul><p>Even with SMT, straying from the golden path and using newer SMTLIB features tends to lead to frustration, e.g. bugs and lacking support in solver APIs.</p><h1 id=rust-will-find-a-way>Rust will find a way</h1><p>My original Python implementation got unreasonably slow past a certain point.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$ ./run.sh examples/ode.py
</span></span><span class=line><span class=cl>music4.so not built, using Python implementation
</span></span><span class=line><span class=cl>Constraint (melody)             0.01s
</span></span><span class=line><span class=cl>Constraint (basics)             0.03s
</span></span><span class=line><span class=cl>Constraint (rhythm)             0.00s
</span></span><span class=line><span class=cl>Constraint (same ending rhythm) 0.00s
</span></span><span class=line><span class=cl>Constraint (no breathing)       0.00s
</span></span><span class=line><span class=cl>Constraint (no breathing)       0.00s
</span></span><span class=line><span class=cl>Constraint (no breathing)       0.00s
</span></span><span class=line><span class=cl>Constraint (simultaneity)       1.51s
</span></span><span class=line><span class=cl>Constraint (consonance)         23.50s
</span></span><span class=line><span class=cl>Constraint (melody intervals)   0.10s
</span></span><span class=line><span class=cl>Constraint (note durations)     0.04s
</span></span><span class=line><span class=cl>Constraint (chord durations)    0.02s
</span></span><span class=line><span class=cl>Solving                         59.69s
</span></span><span class=line><span class=cl>sat
</span></span><span class=line><span class=cl>Ode to Joy.musicxml
</span></span><span class=line><span class=cl>Ode to Joy.abc
</span></span></code></pre></div><p>It turns out that Z3's Python API <a href=https://stackoverflow.com/a/26385910>has significant overhead</a>.
I found other examples where just adding the constraints took 5s, but solving them was instant, evidenced by running Z3 on the generated SMTLIB file.</p><p>The usual move from here is to rewrite in Rust, so that's what I did. This significantly improved solving time...</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$ ./run.sh examples/ode.py
</span></span><span class=line><span class=cl>Using music4.so
</span></span><span class=line><span class=cl>Constraints     446.5ms
</span></span><span class=line><span class=cl>Solving         15.1s
</span></span><span class=line><span class=cl>Sat
</span></span><span class=line><span class=cl>Ode to Joy.musicxml
</span></span><span class=line><span class=cl>Ode to Joy.abc
</span></span></code></pre></div><p>... at the cost of having to maintain a second, slightly-different implementation. I would have liked to retire the Python version, but there are reasons to keep it:</p><ul><li>It's significantly more accessible (i.e. why the Z3 Python API is so popular). Having to install a Rust toolchain and wait for Z3 to compile is not great for just trying out someone's code.</li><li>It serves as a reference implementation for the much more verbose Rust version.</li></ul><p>Maybe it could be used for differential testing someday...</p><h1 id=examples>Examples</h1><p>Now for some examples of generated pieces.
The constraints used for these may be found <a href=https://github.com/dariusf/allez>here</a>.
Feel free to tweak them and generate of your own pieces.</p><h2 id=tabula-rasa>Tabula rasa</h2><p>In the absence of other constraints, all we have here is that intervals in consecutive notes have pleasing, familiar intervals.</p><h2 id=countermelody>Countermelody</h2><h2 id=harmonization>Harmonization</h2><span id=ode>X: 1
T: Ode to Joy
L: 1/16
K: C
M: 4/4
Q: 1/4=110
V: sop name="S"
V: alto name="A"
V: tenor name="T" clef=treble-8
V: bass name="B" clef=bass middle=d transpose=-24
%
V: sop
E4 E4 F4 G4 G4 F4 E4 D4 C4 C4 D4 E4 E6 D2 D8 E4 E4 F4 G4 G4 F4 E4 D4 C4 C4 D4 E4 D6 C2 C8
V: alto
B8 A4 G8 D4 A,8 A,8 G,6 B,6 E2 B2 B2 A2 G2 A8 E2 F2 F2 G4 G2 G2 A8 D2 D2 C6 E2 B4 E4 A6 E2 C8
V: tenor
G8 d2 d2 g8 d4 A4 D2 A2 E2 A6 d2 G2 C2 G8 d4 a2 b2 a2 a8 d4 G8 A2 F2 C4 G4 c8 B8 A6 e2 e8
V: bass
"iii" G8 "ii" d8 "V" d8 "ii" A8 "vi" A8 "ii" B4 "vi" c2 "V" g8 "I" d4 "iii" a2 "V" d'4 "ii" c'8 "V" f4 "ii" g4 "vi" d4 "vi" A4 "ii" A2 "V" A2 "iii" d4 "V" c6 "ii" c2 "vi" G2 "V" d2 "vi" g2 "V" g2 "iii" d'2 "iii" d'2 "ii" d'2 "vi" c'2 "I" c'8
</span><script>renderMusicIn("ode")</script><h1 id=related-work>Related work</h1><p>The most closely related works are the constraint programming libraries <a href=https://github.com/tanders/strasheela>Strasheela (Oz)</a> (which I took a lot of inspiration from) and <a href=https://github.com/tanders/cluster-engine>cluster-engine (Common Lisp)</a>. Others are <a href=https://github.com/slemouton/gecodeMCP>gecodeMCP (C++)</a> and <a href=https://github.com/openmusic-project/Clouds>OMClouds (Common Lisp)</a>.</p><p>I have not yet tried most of them, but given their maturity I'm assuming they can handle all these benchmarks.
One salient difference is the language the user must write constraints in (annotated above).</p><p>Another is that the use of SMT allows <em>arbitrary</em> constraints in standard theories (e.g. LIA) to be expressed.
With constraint solvers, the set of usable constraints is typically <a href=https://sofdem.github.io/gccat/>large but limited</a> (no arbitrary disjunction!) and non-uniform across solvers, but solvable with more efficient specialized algorithms.
They may be more appropriate if the set of constraints required is well-understood (not the case yet, in this work).</p><p>Many of these libraries have been integrated into composition IDEs, such as
<a href=https://opusmodus.com/>Opusmodus</a>, <a href=https://openmusic-project.github.io/>OpenMusic</a>, and <a href=https://en.wikiversity.org/wiki/Music/Software/PWGL>PWGL (seemingly defunct, links no longer work)</a>. <a href=https://github.com/blapiere/Rhythm-Box>Rhythm-Box</a> and <a href=https://www.info.ucl.ac.be/~pvr/SPROCKEELS_68641400_2022.pdf>Melodizer</a> (mentioned earlier) are components of OpenMusic.</p><p>Other systems this work was inspired by are <a href=https://arxiv.org/abs/1006.4948>ANTON</a>, which uses ASP for harmonization, <a href=https://github.com/halfaya/MusicTools/blob/master/doc/farm22/abstract.pdf>MusicTools</a>, an Agda library which also discharges musical synthesis via SMT, and <a href=https://drive.google.com/file/d/18xE9Jmh2gq-KrIGmbKObxiFSWuoFqsi1/view>Type-Guided Music Composition</a>, an approach which uses weighted refinement types to validate and synthesize music.</p><h1 id=conclusion>Conclusion</h1><p>This is very much early work.
Nevertheless, I'm publishing it to document my progress, get feedback, and allow others to play with the code.
I've spent way too long working on this in isolation.</p><p>Here are a few ideas for how this work could be brought forward.</p><p>The prototype is currently limited to a single key (C maj) and 6 diatonic chords.
Lifting these restrictions or improving the background theory (e.g. richer melody constraints, rhythmic constraints, more contrapunctal rules) would be great.
The potential of SMT solvers (optimization, soft constraints, unsat cores, alternative solvers) could be further exploited.
The system could be applied to more interesting musical puzzles, e.g. fugues, which would likely require new kinds of global constraints.</p><p>While it would be nice to integrate this into a composition IDE or build a new one for casual creators, I have no immediate plans to do so.
There are many <a href=http://erichorvitz.com/chi99horvitz.pdf>UX problems</a> to be solved for a good UI design.
In particular, the UI should be cognizant of the limitations and potential of the use of SMT, and should provide the right affordances for creating music without requiring users to dive into the underlying theory.
The NP-hardness of SMT suggests that the UI should provide a means to localize solving.</p><p>The stability of produced outputs is another consideration: it is possible that the unconstrained parts of the piece could be entirely different between two solver calls. This would be a problem if users are working with the system in true mixed-initiative fashion, using its feedback to refine a synthesized piece.
<a href=https://github.com/srcclr/sapling>My previous work</a> suffers from the same problem and utilized manual pinning of parts of the solution to mitigate this, but I'm sure there is a better way.</p><p>Code <a href=https://github.com/dariusf/allez>here</a>.</p></div><p></p></main><footer></footer></body></html>